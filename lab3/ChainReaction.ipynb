{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b20a81-25eb-4dfe-ada9-940e014ce8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 parole del dizionario pulito:\n",
      "1. abbagliare\n",
      "2. abbaglio\n",
      "3. abbaiare\n",
      "4. abbandonare\n",
      "5. abbandonarsi\n",
      "6. abbandono\n",
      "7. abbassamento\n",
      "8. abbassare\n",
      "9. abbattere\n",
      "10. abbigliamento\n"
     ]
    }
   ],
   "source": [
    "#Ho scaricato il dizionario delle collocazioni da qui https://downloads.freemdict.com/%E5%B0%9A%E6%9C%AA%E6%95%B4%E7%90%86/%E5%85%B1%E4%BA%AB2020.5.11/content/4_others/italian/Dizionario%20delle%20collocazioni%20Le%20combinazioni%20delle%20parole%20in%20italiano/\n",
    "#Utilizzato questo tool per convertirlo da .mdx a .json, https://github.com/ilius/pyglossary/tree/master\n",
    "\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "#Ripuliamo le entrate del dizionario\n",
    "STOPWORDS = {\n",
    "    \"a\", \"ad\", \"al\", \"allo\", \"ai\", \"agli\", \"all\", \"alla\", \"alle\", \"ha\", \"fa\", \"con\", \"col\", \"coi\", \"v\", \"verbi\", \"è\", \"nm\", \"f\", \n",
    "    \"all'\", \"e\", \"dà\", \"da\", \"dal\", \"dallo\", \"dai\", \"dagli\", \"dall\", \"dall'\", \"dalla\", \"dalle\", \"di\", \"del\", \"dello\", \"dei\", \n",
    "    \"degli\", \"dell\", \"dell'\", \"della\", \"delle\", \"in\", \"nel\", \"nello\", \"nei\", \"negli\", \"nell\", \"nell'\", \"nella\", \"nelle\", \"su\", \n",
    "    \"sul\", \"sullo\", \"sui\", \"sugli\", \"sull\", \"sull'\", \"sulla\", \"sulle\", \"dell'\", \"per\", \"tra\", \"fra\", \"avverbi\", \"pl\", \"ns\", \"s\", \n",
    "    \"aggettivo\", \"più\", \"aggettivi\", \"verbo\", \"agg\", \"qlco\", \"qlcu\", \"all\", \"si\", \"inv\", \"non\", \"ecc\", \"il\", \"lo\", \"la\", \"i\", \n",
    "    \"gli\", \"le\", \"l\", \"un\", \"uno\", \"una\", \"un'\", \"et\", \"na\", \"suo\", \"sua\", \"e\", \"ed\", \"o\", \n",
    "    \"od\", \"ma\", \"né\", \"ne\", \"che\", \"se\", \"quando\", \"come\", \"quale\", \"quali\", \"cui\", \"nf\", \"all'\", \"complemento\", \"complementi\", \n",
    "    \"AVVERBI\", \"AGGETTIVI\", \"VERBO\", \"COMPLEMENTO\", \"SOGGETTO\", \"soggetto\"\n",
    "}\n",
    "\n",
    "def clean_entry(lemma, raw_html):\n",
    "    #Rimuovo i <link>\n",
    "    raw_html = re.sub(r'<link[^>]+>', '', raw_html)\n",
    "\n",
    "    # arsing HTML\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    #Trovo solo parole e locuzioni\n",
    "    words = re.findall(r\"[a-zA-ZàèéìòùÀÈÉÌÒÙ']+\", text)\n",
    "\n",
    "    #Pulizia\n",
    "    cleaned = []\n",
    "    for w in words:\n",
    "        wl = w.lower()\n",
    "        #Ignoro lo stesso lemma, le stopwords e tutte le parole\n",
    "        #con lunghezza <= 2\n",
    "        if wl == lemma.lower() or wl in STOPWORDS or len(wl) <= 2 or len(wl) >= 10:\n",
    "            continue\n",
    "        cleaned.append(wl)\n",
    "    return cleaned\n",
    "\n",
    "#Carico il JSON di partenza\n",
    "with open(\"dizionari/dizionario.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#Creo un dizionario unificato\n",
    "cleaned_dict = defaultdict(list)\n",
    "\n",
    "for lemma, html in data.items():\n",
    "    #Salto chiavi che contengono ##\n",
    "    if \"##\" in lemma:\n",
    "        continue\n",
    "\n",
    "    #Rimuovo eventuali numeri tipo mente(2) → mente\n",
    "    base_lemma = re.sub(r'\\(\\d+\\)', '', lemma)\n",
    "\n",
    "    cleaned_words = clean_entry(base_lemma, html)\n",
    "\n",
    "    # Unifico tutte le parole dello stesso lemma\n",
    "    cleaned_dict[base_lemma].extend(cleaned_words)\n",
    "\n",
    "#Rimuovo duplicati\n",
    "cleaned_dict = {k: list(set(v)) for k, v in cleaned_dict.items()}\n",
    "\n",
    "#Salvo il risultato\n",
    "with open(\"dizionari/dizionario_pulito.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Prime 10 parole del dizionario pulito:\")\n",
    "for i, word in enumerate(list(cleaned_dict.keys())[:10], start=1):\n",
    "    print(f\"{i}. {word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014537fa-50a4-4f34-956e-f71625566c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 parole del dizionario:\n",
      "1. abbagliare\n",
      "2. offuscare\n",
      "3. illudere\n",
      "4. vista\n",
      "5. abbaglio\n",
      "6. fatale\n",
      "7. totale\n",
      "8. lieve\n",
      "9. solito\n",
      "10. tremendo\n"
     ]
    }
   ],
   "source": [
    "#Osservando il file .json però mi sono accorto che tutte le parole dovevano essere\n",
    "#collegate biunivocamente, in quanto solitamente per gli autori del gioco\n",
    "#nervi->saldi oppure saldi-nervi hanno lo stesso valore.\n",
    "\n",
    "#Carica il dizionario pulito\n",
    "with open(\"dizionari/dizionario_pulito.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#Dizionario aumentato\n",
    "augmented = defaultdict(list)\n",
    "\n",
    "for lemma, words in data.items():\n",
    "    #aggiungo la relazione originale\n",
    "    augmented[lemma].extend(words)\n",
    "    \n",
    "    #creo le relazioni inverse\n",
    "    for w in words:\n",
    "        if lemma not in augmented[w]:  #evita duplicati\n",
    "            augmented[w].append(lemma)\n",
    "\n",
    "#Rimuovo duplicati in ogni lista\n",
    "augmented = {k: list(set(v)) for k, v in augmented.items()}\n",
    "\n",
    "#Salvo il risultato\n",
    "with open(\"dizionari/dizionario_b.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(augmented, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Prime 10 parole del dizionario:\")\n",
    "for i, word in enumerate(list(augmented.keys())[:10], start=1):\n",
    "    print(f\"{i}. {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fb3afc-eadd-48e7-bb27-34b2790568c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catena 1: cuocere -> bene -> prezioso -> documento -> reperire -> materiale\n",
      "Catena 2: case -> gruppo -> pressione -> sentire -> vocazione -> scoprire\n",
      "Catena 3: medicina -> tollerare -> sopraffazione -> atto -> crudele -> sovrano\n",
      "Catena 4: comunale -> imposta -> pagare -> cifra -> denaro -> usare\n",
      "Catena 5: diocesano -> palazzo -> popolare -> canzone -> d'autore -> quadro\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Qui ho scritto una fuzione per valutare la bontà delle catene che si potrebbero generare\n",
    "def normalize(word):\n",
    "    word = word.lower()\n",
    "    word = word.strip(\",.!?;:'\")  #Rimuove punteggiatura\n",
    "    return word\n",
    "\n",
    "def catene_logiche_no_dup(dizionario, n_catene=5, lunghezza_catena=6):\n",
    "    catene_generati = []\n",
    "\n",
    "    keys = list(dizionario.keys())\n",
    "\n",
    "    for _ in range(n_catene):\n",
    "        catena = []\n",
    "        normalizzati = set()  #traccia dei lemmi già presenti\n",
    "        current = random.choice(keys)\n",
    "        catena.append(current)\n",
    "        normalizzati.add(normalize(current))\n",
    "\n",
    "        for _ in range(lunghezza_catena - 1):\n",
    "            possibili = dizionario.get(current, [])\n",
    "            #filtra parole che non sono già normalizzate nella catena\n",
    "            possibili = [w for w in possibili if normalize(w) not in normalizzati]\n",
    "            if not possibili:\n",
    "                break\n",
    "            next_word = random.choice(possibili)\n",
    "            catena.append(next_word)\n",
    "            normalizzati.add(normalize(next_word))\n",
    "\n",
    "            #aggiorna current solo se next_word è chiave nel dizionario\n",
    "            if next_word in dizionario:\n",
    "                current = next_word\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        catene_generati.append(catena)\n",
    "\n",
    "    #stampa catene\n",
    "    for i, c in enumerate(catene_generati, 1):\n",
    "        print(f\"Catena {i}: {' -> '.join(c)}\")\n",
    "\n",
    "#Carica il JSON bidirezionale\n",
    "with open(\"dizionari/dizionario_b.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(catene_logiche_no_dup(data, n_catene=5, lunghezza_catena=6))\n",
    "\n",
    "#Le catene diciamo che hanno alti e bassi:\n",
    "#i legami tra le parole ci sono sempre ma a volte per capirli\n",
    "#bisogna rifletterci, quelli del programma televisivo sono molto\n",
    "#più immediati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a70ab9-9eee-4bed-9530-9fd5542376d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 voci del dataset:\n",
      "1: incrinare -> p [ANSWER] pacatezza\n",
      "2: lucido -> p [ANSWER] profilo\n",
      "3: accanimento -> t [ANSWER] testardo\n",
      "4: regressione -> p [ANSWER] profonda\n",
      "5: incontro -> f [ANSWER] fiasco\n",
      "6: cellulare -> a [ANSWER] ammasso\n",
      "7: disoccupazione -> r [ANSWER] ridurre\n",
      "8: irresponsabile -> v [ANSWER] veramente\n",
      "9: internet -> c [ANSWER] collegamento\n",
      "10: dogana -> b [ANSWER] bloccare\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "#Carica il JSON\n",
    "with open(\"dizionari/dizionario_b.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "num_examples = 50000\n",
    "train_examples = []\n",
    "\n",
    "#Lista delle chiavi non vuote\n",
    "valid_keys = [k for k, v in data.items() if v]  #solo chiavi con almeno una parola\n",
    "\n",
    "for _ in range(num_examples):\n",
    "    #Scelgo una chiave principale casuale tra quelle valide\n",
    "    start_word = random.choice(valid_keys)\n",
    "    \n",
    "    #Scelgo una parola target casuale all'interno della lista\n",
    "    target_word = random.choice(data[start_word])\n",
    "    \n",
    "    #Prendo la prima lettera del target\n",
    "    first_letter = target_word[0]\n",
    "    \n",
    "    #Creo il testo con un token speciale (intendo utilizzare gpt2)\n",
    "    text = f\"{start_word} -> {first_letter} [ANSWER] {target_word}\"\n",
    "    \n",
    "    train_examples.append({\"text\": text})\n",
    "\n",
    "#Mostra le prime 10 voci del dataset\n",
    "print(\"Prime 10 voci del dataset:\")\n",
    "for i, example in enumerate(train_examples[:10]):\n",
    "    print(f\"{i+1}: {example['text']}\")\n",
    "\n",
    "#Salva in un file JSON\n",
    "with open(\"dizionari/generated_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_examples, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee0aeb-a031-42dd-98da-96801d2dc1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061279e292bc4bf3830f959684015f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26f2cd6e62e46c99c8f811f2fdfcacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120439/1230961968.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "/home/cosimo/anaconda3/envs/chain/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1480/15000 11:51 < 1:48:28, 2.08 it/s, Epoch 0.30/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset                   \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model    \n",
    "import torch                                  \n",
    "\n",
    "dataset = Dataset.from_list(train_examples)  \n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "#Carica tokenizer preaddestrato per distilgpt2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "#Aggiunge token speciale [ANSWER] che useremo per separare input e output nel training\n",
    "special_tokens_dict = {'additional_special_tokens': ['[ANSWER]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "#Imposta il token di padding uguale a eos_token perché GPT2 non ha un pad_token \n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "\n",
    "#Carica modello pre-addestrato distilgpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "#Aggiorna la dimensione del vocabolario del modello per includere i token speciali\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#Assicura che il modello sappia quale token usare come pad\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                 \n",
    "    lora_alpha=32,        \n",
    "    target_modules=[\"c_attn\"],  # moduli GPT2 dove applicare LoRA\n",
    "    lora_dropout=0.1,     \n",
    "    bias=\"none\",          \n",
    "    task_type=\"CAUSAL_LM\" \n",
    ")\n",
    "\n",
    "#Applica LoRA al modello\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"\n",
    "    Tokenizza batch di testi, tronca o riempie a max_length=32.\n",
    "    Copia input_ids in labels per il Language Modeling.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=32\n",
    "    )\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()  # etichette = input_ids\n",
    "    return encoding\n",
    "\n",
    "#Applica la tokenizzazione a tutto il dataset\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            \n",
    "    overwrite_output_dir=True,         \n",
    "    eval_strategy=\"epoch\",             \n",
    "    learning_rate=2e-4,                \n",
    "    per_device_train_batch_size=8,     \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,                \n",
    "    weight_decay=0.01,                \n",
    "    logging_steps=50,                  \n",
    "    save_strategy=\"epoch\"             \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],   \n",
    "    eval_dataset=dataset[\"test\"],     \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()                        \n",
    "\n",
    "save_path = \"./finetuned-distilgpt2-lora\"\n",
    "trainer.save_model(save_path)          #salva modello fine-tuned\n",
    "tokenizer.save_pretrained(save_path)   #salva tokenizer aggiornato\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(save_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "\n",
    "prompt = \"cane -> l [ANSWER]\"         #prompt di esempio\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=3,                 \n",
    "    num_return_sequences=3,           #tre alternative\n",
    "    do_sample=False,                    #sampling casuale disattivato\n",
    "    top_k=50,                          #considera solo i top 50 token più probabili\n",
    "    temperature=1                   #controlla la \"creatività\" (+alto +creativo)\n",
    ")\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"Tre predizioni candidate:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    pred = text.replace(prompt, \"\").strip()\n",
    "    print(f\"{i+1}) {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa0bb41-7b89-43f6-8c80-edab6a2a73f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Carico modello da ./finetuned-distilgpt2...\n",
      "✅ Modello caricato!\n",
      "\n",
      "💬 Modalità interattiva: inserisci una parola e poi l'iniziale successiva.\n",
      "   Esempio: parola = cane, iniziale = l -> cane -> l [ANSWER]\n",
      "   Digita 'exit' in qualunque momento per uscire.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👉 Inserisci la prima parola:  occhi\n",
      "👉 Inserisci l'iniziale della parola successiva:  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 Predizioni per: occhi -> l\n",
      "1) occhi -> l  legito\n",
      "2) occhi -> l  leggia\n",
      "3) occhi -> l  letturo\n",
      "4) occhi -> l  lamponte\n",
      "5) occhi -> l  lato\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   Digita \u001b[39m\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in qualunque momento per uscire.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m#Primo input: parola\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     first_word = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m👉 Inserisci la prima parola: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m first_word.lower() == \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     37\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m👋 Uscita dal programma.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lab2/lib/python3.13/site-packages/ipykernel/kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/lab2/lib/python3.13/site-packages/ipykernel/kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "save_path = \"./finetuned-distilgpt2\"\n",
    "\n",
    "print(f\"🔄 Carico modello da {save_path}...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(save_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "print(\"✅ Modello caricato!\\n\")\n",
    "\n",
    "def generate_predictions(prompt, num_predictions=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=4,          # genera fino a 3 token per completare\n",
    "        num_return_sequences=num_predictions, \n",
    "        do_sample=True,            #sampling casuale\n",
    "        top_k=50,                  #scegli tra i migliori 50\n",
    "        repetition_penalty=3.0,      #penalità per la ripetizione\n",
    "        temperature=0.9            #regola la diversità\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        pred = text.replace(prompt, \"\").strip()\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "print(\"💬 Modalità interattiva: inserisci una parola e poi l'iniziale successiva.\")\n",
    "print(\"   Esempio: parola = cane, iniziale = l -> cane -> l [ANSWER]\")\n",
    "print(\"   Digita 'exit' in qualunque momento per uscire.\\n\")\n",
    "\n",
    "while True:\n",
    "    #Primo input: parola\n",
    "    first_word = input(\"👉 Inserisci la prima parola: \").strip()\n",
    "    if first_word.lower() == \"exit\":\n",
    "        print(\"👋 Uscita dal programma.\")\n",
    "        break\n",
    "\n",
    "    #Secondo input: iniziale della parola target\n",
    "    initial = input(\"👉 Inserisci l'iniziale della parola successiva: \").strip()\n",
    "    if initial.lower() == \"exit\":\n",
    "        print(\"👋 Uscita dal programma.\")\n",
    "        break\n",
    "\n",
    "    #Costruisci il prompt\n",
    "    prompt = f\"{first_word} -> {initial} [ANSWER]\"\n",
    "\n",
    "    #Genera predizioni\n",
    "    preds = generate_predictions(prompt, num_predictions=5)\n",
    "\n",
    "    #Stampa i risultati\n",
    "    print(f\"\\n🔮 Predizioni per: {first_word} -> {initial}\")\n",
    "    for i, p in enumerate(preds, 1):\n",
    "        print(f\"{i}) {p}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
