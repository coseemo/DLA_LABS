{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b20a81-25eb-4dfe-ada9-940e014ce8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 parole del dizionario pulito:\n",
      "1. abbagliare\n",
      "2. abbaglio\n",
      "3. abbaiare\n",
      "4. abbandonare\n",
      "5. abbandonarsi\n",
      "6. abbandono\n",
      "7. abbassamento\n",
      "8. abbassare\n",
      "9. abbattere\n",
      "10. abbigliamento\n"
     ]
    }
   ],
   "source": [
    "#Ho scaricato il dizionario delle collocazioni da qui https://downloads.freemdict.com/%E5%B0%9A%E6%9C%AA%E6%95%B4%E7%90%86/%E5%85%B1%E4%BA%AB2020.5.11/content/4_others/italian/Dizionario%20delle%20collocazioni%20Le%20combinazioni%20delle%20parole%20in%20italiano/\n",
    "#Utilizzato questo tool per convertirlo da .mdx a .json, https://github.com/ilius/pyglossary/tree/master\n",
    "\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "#Ripuliamo le entrate del dizionario\n",
    "STOPWORDS = {\n",
    "    \"a\", \"ad\", \"al\", \"allo\", \"ai\", \"agli\", \"all\", \"alla\", \"alle\", \"ha\", \"fa\", \"con\", \"col\", \"coi\", \"v\", \"verbi\", \"è\", \"nm\", \"f\", \n",
    "    \"all'\", \"e\", \"dà\", \"da\", \"dal\", \"dallo\", \"dai\", \"dagli\", \"dall\", \"dall'\", \"dalla\", \"dalle\", \"di\", \"del\", \"dello\", \"dei\", \n",
    "    \"degli\", \"dell\", \"dell'\", \"della\", \"delle\", \"in\", \"nel\", \"nello\", \"nei\", \"negli\", \"nell\", \"nell'\", \"nella\", \"nelle\", \"su\", \n",
    "    \"sul\", \"sullo\", \"sui\", \"sugli\", \"sull\", \"sull'\", \"sulla\", \"sulle\", \"dell'\", \"per\", \"tra\", \"fra\", \"avverbi\", \"pl\", \"ns\", \"s\", \n",
    "    \"aggettivo\", \"più\", \"aggettivi\", \"verbo\", \"agg\", \"qlco\", \"qlcu\", \"all\", \"si\", \"inv\", \"non\", \"ecc\", \"il\", \"lo\", \"la\", \"i\", \n",
    "    \"gli\", \"le\", \"l\", \"un\", \"uno\", \"una\", \"un'\", \"et\", \"na\", \"suo\", \"sua\", \"e\", \"ed\", \"o\", \n",
    "    \"od\", \"ma\", \"né\", \"ne\", \"che\", \"se\", \"quando\", \"come\", \"quale\", \"quali\", \"cui\", \"nf\", \"all'\", \"complemento\", \"complementi\", \n",
    "    \"AVVERBI\", \"AGGETTIVI\", \"VERBO\", \"COMPLEMENTO\", \"SOGGETTO\", \"soggetto\"\n",
    "}\n",
    "\n",
    "def clean_entry(lemma, raw_html):\n",
    "    #Rimuovo i <link>\n",
    "    raw_html = re.sub(r'<link[^>]+>', '', raw_html)\n",
    "\n",
    "    # arsing HTML\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    #Trovo solo parole e locuzioni\n",
    "    words = re.findall(r\"[a-zA-ZàèéìòùÀÈÉÌÒÙ']+\", text)\n",
    "\n",
    "    #Pulizia\n",
    "    cleaned = []\n",
    "    for w in words:\n",
    "        wl = w.lower()\n",
    "        #Ignoro lo stesso lemma, le stopwords e tutte le parole\n",
    "        #con lunghezza <= 2\n",
    "        if wl == lemma.lower() or wl in STOPWORDS or len(wl) <= 2 or len(wl) >= 10:\n",
    "            continue\n",
    "        cleaned.append(wl)\n",
    "    return cleaned\n",
    "\n",
    "#Carico il JSON di partenza\n",
    "with open(\"dizionari/dizionario.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#Creo un dizionario unificato\n",
    "cleaned_dict = defaultdict(list)\n",
    "\n",
    "for lemma, html in data.items():\n",
    "    #Salto chiavi che contengono ##\n",
    "    if \"##\" in lemma:\n",
    "        continue\n",
    "\n",
    "    #Rimuovo eventuali numeri tipo mente(2) → mente\n",
    "    base_lemma = re.sub(r'\\(\\d+\\)', '', lemma)\n",
    "\n",
    "    cleaned_words = clean_entry(base_lemma, html)\n",
    "\n",
    "    # Unifico tutte le parole dello stesso lemma\n",
    "    cleaned_dict[base_lemma].extend(cleaned_words)\n",
    "\n",
    "#Rimuovo duplicati\n",
    "cleaned_dict = {k: list(set(v)) for k, v in cleaned_dict.items()}\n",
    "\n",
    "#Salvo il risultato\n",
    "with open(\"dizionari/dizionario_pulito.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Prime 10 parole del dizionario pulito:\")\n",
    "for i, word in enumerate(list(cleaned_dict.keys())[:10], start=1):\n",
    "    print(f\"{i}. {word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014537fa-50a4-4f34-956e-f71625566c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 parole del dizionario:\n",
      "1. abbagliare\n",
      "2. offuscare\n",
      "3. illudere\n",
      "4. vista\n",
      "5. abbaglio\n",
      "6. fatale\n",
      "7. totale\n",
      "8. lieve\n",
      "9. solito\n",
      "10. tremendo\n"
     ]
    }
   ],
   "source": [
    "#Osservando il file .json però mi sono accorto che tutte le parole dovevano essere\n",
    "#collegate biunivocamente, in quanto solitamente per gli autori del gioco\n",
    "#nervi->saldi oppure saldi-nervi hanno lo stesso valore.\n",
    "\n",
    "#Carica il dizionario pulito\n",
    "with open(\"dizionari/dizionario_pulito.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#Dizionario aumentato\n",
    "augmented = defaultdict(list)\n",
    "\n",
    "for lemma, words in data.items():\n",
    "    #aggiungo la relazione originale\n",
    "    augmented[lemma].extend(words)\n",
    "    \n",
    "    #creo le relazioni inverse\n",
    "    for w in words:\n",
    "        if lemma not in augmented[w]:  #evita duplicati\n",
    "            augmented[w].append(lemma)\n",
    "\n",
    "#Rimuovo duplicati in ogni lista\n",
    "augmented = {k: list(set(v)) for k, v in augmented.items()}\n",
    "\n",
    "#Salvo il risultato\n",
    "with open(\"dizionari/dizionario_b.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(augmented, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Prime 10 parole del dizionario:\")\n",
    "for i, word in enumerate(list(augmented.keys())[:10], start=1):\n",
    "    print(f\"{i}. {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fb3afc-eadd-48e7-bb27-34b2790568c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catena 1: cuocere -> bene -> prezioso -> documento -> reperire -> materiale\n",
      "Catena 2: case -> gruppo -> pressione -> sentire -> vocazione -> scoprire\n",
      "Catena 3: medicina -> tollerare -> sopraffazione -> atto -> crudele -> sovrano\n",
      "Catena 4: comunale -> imposta -> pagare -> cifra -> denaro -> usare\n",
      "Catena 5: diocesano -> palazzo -> popolare -> canzone -> d'autore -> quadro\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Qui ho scritto una fuzione per valutare la bontà delle catene che si potrebbero generare\n",
    "def normalize(word):\n",
    "    word = word.lower()\n",
    "    word = word.strip(\",.!?;:'\")  #Rimuove punteggiatura\n",
    "    return word\n",
    "\n",
    "def catene_logiche_no_dup(dizionario, n_catene=5, lunghezza_catena=6):\n",
    "    catene_generati = []\n",
    "\n",
    "    keys = list(dizionario.keys())\n",
    "\n",
    "    for _ in range(n_catene):\n",
    "        catena = []\n",
    "        normalizzati = set()  #traccia dei lemmi già presenti\n",
    "        current = random.choice(keys)\n",
    "        catena.append(current)\n",
    "        normalizzati.add(normalize(current))\n",
    "\n",
    "        for _ in range(lunghezza_catena - 1):\n",
    "            possibili = dizionario.get(current, [])\n",
    "            #filtra parole che non sono già normalizzate nella catena\n",
    "            possibili = [w for w in possibili if normalize(w) not in normalizzati]\n",
    "            if not possibili:\n",
    "                break\n",
    "            next_word = random.choice(possibili)\n",
    "            catena.append(next_word)\n",
    "            normalizzati.add(normalize(next_word))\n",
    "\n",
    "            #aggiorna current solo se next_word è chiave nel dizionario\n",
    "            if next_word in dizionario:\n",
    "                current = next_word\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        catene_generati.append(catena)\n",
    "\n",
    "    #stampa catene\n",
    "    for i, c in enumerate(catene_generati, 1):\n",
    "        print(f\"Catena {i}: {' -> '.join(c)}\")\n",
    "\n",
    "#Carica il JSON bidirezionale\n",
    "with open(\"dizionari/dizionario_b.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(catene_logiche_no_dup(data, n_catene=5, lunghezza_catena=6))\n",
    "\n",
    "#Le catene diciamo che hanno alti e bassi:\n",
    "#i legami tra le parole ci sono sempre ma a volte per capirli\n",
    "#bisogna rifletterci, quelli del programma televisivo sono molto\n",
    "#più immediati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a70ab9-9eee-4bed-9530-9fd5542376d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 10 voci del dataset:\n",
      "1: incrinare -> p [ANSWER] pacatezza\n",
      "2: lucido -> p [ANSWER] profilo\n",
      "3: accanimento -> t [ANSWER] testardo\n",
      "4: regressione -> p [ANSWER] profonda\n",
      "5: incontro -> f [ANSWER] fiasco\n",
      "6: cellulare -> a [ANSWER] ammasso\n",
      "7: disoccupazione -> r [ANSWER] ridurre\n",
      "8: irresponsabile -> v [ANSWER] veramente\n",
      "9: internet -> c [ANSWER] collegamento\n",
      "10: dogana -> b [ANSWER] bloccare\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "#Carica il JSON\n",
    "with open(\"dizionari/dizionario_b.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "num_examples = 50000\n",
    "train_examples = []\n",
    "\n",
    "#Lista delle chiavi non vuote\n",
    "valid_keys = [k for k, v in data.items() if v]  #solo chiavi con almeno una parola\n",
    "\n",
    "for _ in range(num_examples):\n",
    "    #Scelgo una chiave principale casuale tra quelle valide\n",
    "    start_word = random.choice(valid_keys)\n",
    "    \n",
    "    #Scelgo una parola target casuale all'interno della lista\n",
    "    target_word = random.choice(data[start_word])\n",
    "    \n",
    "    #Prendo la prima lettera del target\n",
    "    first_letter = target_word[0]\n",
    "    \n",
    "    #Creo il testo con un token speciale (intendo utilizzare gpt2)\n",
    "    text = f\"{start_word} -> {first_letter} [ANSWER] {target_word}\"\n",
    "    \n",
    "    train_examples.append({\"text\": text})\n",
    "\n",
    "#Mostra le prime 10 voci del dataset\n",
    "print(\"Prime 10 voci del dataset:\")\n",
    "for i, example in enumerate(train_examples[:10]):\n",
    "    print(f\"{i+1}: {example['text']}\")\n",
    "\n",
    "#Salva in un file JSON\n",
    "with open(\"dizionari/generated_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_examples, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beee0aeb-a031-42dd-98da-96801d2dc1de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosimo/anaconda3/envs/chain/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function tokenize at 0x7f84c7f1c220> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a72f366eaa34481bb5a39df92ea29eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#Applica la tokenizzazione a tutto il dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m training_args = TrainingArguments(\n\u001b[32m     61\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,            \n\u001b[32m     62\u001b[39m     overwrite_output_dir=\u001b[38;5;28;01mTrue\u001b[39;00m,         \n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     save_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m             \n\u001b[32m     71\u001b[39m )\n\u001b[32m     73\u001b[39m trainer = Trainer(\n\u001b[32m     74\u001b[39m     model=model,\n\u001b[32m     75\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     tokenizer=tokenizer\n\u001b[32m     79\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/datasets/dataset_dict.py:946\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    944\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    968\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/datasets/arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/datasets/arrow_dataset.py:3318\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3317\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/datasets/arrow_dataset.py:3674\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3673\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3674\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3676\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/datasets/arrow_dataset.py:3624\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3622\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3623\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3624\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/datasets/arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3546\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(batch):\n\u001b[32m     43\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    Tokenizza batch di testi, tronca o riempie a max_length=32.\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m    Copia input_ids in labels per il Language Modeling.\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     encoding = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     encoding[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = encoding[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].copy()  \u001b[38;5;66;03m# etichette = input_ids\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2911\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2909\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2910\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2911\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2913\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2999\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2994\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2995\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2996\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2997\u001b[39m         )\n\u001b[32m   2998\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2999\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3021\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   3022\u001b[39m         text=text,\n\u001b[32m   3023\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3041\u001b[39m         **kwargs,\n\u001b[32m   3042\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3200\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3191\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3192\u001b[39m     padding=padding,\n\u001b[32m   3193\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3197\u001b[39m     **kwargs,\n\u001b[32m   3198\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3218\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/tokenization_utils.py:889\u001b[39m, in \u001b[36mPreTrainedTokenizer._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    887\u001b[39m     ids, pair_ids = ids_or_pair_ids\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m second_ids = get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    891\u001b[39m input_ids.append((first_ids, second_ids))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/tokenization_utils.py:854\u001b[39m, in \u001b[36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    855\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2.py:281\u001b[39m, in \u001b[36mGPT2Tokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    278\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m.split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2.py:197\u001b[39m, in \u001b[36mGPT2Tokenizer.bpe\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     bigram = \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbpe_ranks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bigram \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe_ranks:\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/chain/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2.py:197\u001b[39m, in \u001b[36mGPT2Tokenizer.bpe.<locals>.<lambda>\u001b[39m\u001b[34m(pair)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     bigram = \u001b[38;5;28mmin\u001b[39m(pairs, key=\u001b[38;5;28;01mlambda\u001b[39;00m pair: \u001b[38;5;28mself\u001b[39m.bpe_ranks.get(pair, \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bigram \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe_ranks:\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(train_examples)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "special_tokens_dict = {'additional_special_tokens': ['[ANSWER]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    encoding = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=32\n",
    "    )\n",
    "    # Copia gli input_ids in labels\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
    "    return encoding\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,  \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "save_path = \"./finetuned-distilgpt2\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(save_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "\n",
    "prompt = \"cane -> l [ANSWER]\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=3,\n",
    "    num_return_sequences=3,  #tre alternative\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"Tre predizioni candidate:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    pred = text.replace(prompt, \"\").strip()\n",
    "    print(f\"{i+1}) {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0bb41-7b89-43f6-8c80-edab6a2a73f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Carico modello da ./finetuned-distilgpt2...\n",
      "✅ Modello caricato!\n",
      "\n",
      "💬 Modalità interattiva: inserisci una parola e poi l'iniziale successiva.\n",
      "   Esempio: parola = cane, iniziale = l -> cane -> l [ANSWER]\n",
      "   Digita 'exit' in qualunque momento per uscire.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👉 Inserisci la prima parola:  cane\n",
      "👉 Inserisci l'iniziale della parola successiva:  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 Predizioni per: cane -> l\n",
      "1) cane -> l  labbro\n",
      "2) cane -> l  lento\n",
      "3) cane -> l  lettore\n",
      "4) cane -> l  linza\n",
      "5) cane -> l  lineare\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "save_path = \"./finetuned-distilgpt2\"\n",
    "\n",
    "print(f\"🔄 Carico modello da {save_path}...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(save_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "print(\"✅ Modello caricato!\\n\")\n",
    "\n",
    "def generate_predictions(prompt, num_predictions=5):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=4,          # genera fino a 3 token per completare\n",
    "        num_return_sequences=num_predictions, \n",
    "        do_sample=True,            #sampling casuale\n",
    "        top_k=50,                  #scegli tra i migliori 50\n",
    "        repetition_penalty=3.0,      #penalità per la ripetizione\n",
    "        temperature=0.9            #regola la diversità\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        pred = text.replace(prompt, \"\").strip()\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "print(\"💬 Modalità interattiva: inserisci una parola e poi l'iniziale successiva.\")\n",
    "print(\"   Esempio: parola = cane, iniziale = l -> cane -> l [ANSWER]\")\n",
    "print(\"   Digita 'exit' in qualunque momento per uscire.\\n\")\n",
    "\n",
    "while True:\n",
    "    #Primo input: parola\n",
    "    first_word = input(\"👉 Inserisci la prima parola: \").strip()\n",
    "    if first_word.lower() == \"exit\":\n",
    "        print(\"👋 Uscita dal programma.\")\n",
    "        break\n",
    "\n",
    "    #Secondo input: iniziale della parola target\n",
    "    initial = input(\"👉 Inserisci l'iniziale della parola successiva: \").strip()\n",
    "    if initial.lower() == \"exit\":\n",
    "        print(\"👋 Uscita dal programma.\")\n",
    "        break\n",
    "\n",
    "    #Costruisci il prompt\n",
    "    prompt = f\"{first_word} -> {initial} [ANSWER]\"\n",
    "\n",
    "    #Genera predizioni\n",
    "    preds = generate_predictions(prompt, num_predictions=5)\n",
    "\n",
    "    #Stampa i risultati\n",
    "    print(f\"\\n🔮 Predizioni per: {first_word} -> {initial}\")\n",
    "    for i, p in enumerate(preds, 1):\n",
    "        print(f\"{i}) {p}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc8311-fb19-47b6-a552-2075c6d6e685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
