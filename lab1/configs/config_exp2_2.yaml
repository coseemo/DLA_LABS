#Esperimento 2.2: Knowledge Distillation
seed: 0

data:
  batch_size: 128
  validation_split: 20  # percentage
  num_workers: 4

#Modello teacher (più grande)
teacher_model:
  type: "CNN"
  params:
    block_type: "basic"
    layers: [3, 4, 6, 3]
    num_classes: 10
    residual: true
    zero_init_residual: true

#Modello studente (più piccolo)
student_model:
  type: "CNN"
  params:
    block_type: "basic"
    layers: [1, 1, 1, 1]
    num_classes: 10
    residual: false
    zero_init_residual: false

training_teacher:
  epochs: 100
  criterion: "CrossEntropyLoss"
  optimizer: "Adam"
  lr: 0.001
  scheduler: "CosineAnnealingLR"
  
training_student:
  epochs: 50
  criterion: "CrossEntropyLoss"
  optimizer: "Adam"
  lr: 0.03
  scheduler: "CosineAnnealingLR"

distillation:
  temperature: 3.0
  alpha: 0.9

logging:
  project_name: "LAB1-CNN"
  log_gradients: false