#Esperimento 2.2: Knowledge Distillation
seed: 42 

data:
  batch_size: 128
  validation_split: 20  # percentage
  num_workers: 4

#Modello teacher (più grande)
teacher_model:
  type: "CNN"
  params:
    block_type: "basic"
    layers: 9
    num_classes: 10
    residual: true
    zero_init_residual: false

#Modello studente (più piccolo)
student_model:
  type: "CNN"
  params:
    block_type: "basic"
    layers: 3
    num_classes: 10
    residual: true
    zero_init_residual: false

training:
  epochs: 100
  criterion: "CrossEntropyLoss"
  optimizer: "Adam"
  lr: 0.1
  scheduler: "CosineAnnealingLR"

distillation:
  temperature: 3.0
  alpha: 0.7

logging:
  project_name: "LAB1-CNN"
  log_gradients: false